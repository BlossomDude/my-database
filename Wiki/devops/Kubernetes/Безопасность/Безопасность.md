## Аутентификация

Каждый запрос управляется через API Server, вне зависимости используете ли вы команду `kubectl` или напрямую отправляете POST запрос - все они проходят через `kube-apiserver`.
`kube-apiserver` аутентифицирует пользователя перед обработкой запроса к API.

Способы аутентификации:
- Иметь файл с именами и паролями пользователей (или токенами)
	Для данного метода необходимо иметь `csv` файл с пользователями в данном формате:
	`password123, user1, u0001, group1`
	`password123, user2, u0002, group1`
	`password123, user3, u0003, group1`
	4 столбец указывающий группу пользователя - опционален.
	
	После этого необходимо передать при запуске API серверу параметр:
	`--basic-auth-file=user-details.csv`
	Для обращении через curl используйте параметр `-u`:
	`curl -v -k https://master-node:6443/api/v1/pods -u "user1:password123"`
	
	Так же можно использовать вместо пароля - токен. Для этого нужно указать другой параметр для `kube-apiserver`:
	`--token-auth-file=user-tokens.csv`
	Для обращении через curl используйте header:
	`curl https://master-node:6443/api/v1/pods --header "Authorixation: Bearer <token>"

- Использовать сертификаты
- Использовать стороннее ПО (LDAP, Kerberos etc)
---
## Создание сертификатов

В кластере есть два типа сертификатов:
- Клиентские сертификаты - для отправки запросов (shcheduler, controller, proxy и админ)
- Серверные сертификаты - для получение обращений

У каждого компонента кластера (etcd, apiservre, contoller etc.) должны быть свои сертификаты.

>[!info]
>У кластера k8s должен быть как минимум один центр сертификации(CA)

#### Генерация ключей

Для генерации ключей можно использовать команду `openssl`. 
>[!important]
>Сертификаты компонентов kubernetes которые относятся к контрольной плоскости должны иметь `CN=SYSTEM-<name>`
>Узлы - `CN=system:node:<name>`
>[https://github.com/mmumshad/kubernetes-the-hard-way/tree/master/tools](https://github.com/mmumshad/kubernetes-the-hard-way/tree/master/tools)
##### Генерация CA(root) сертификата и ключа:
```bash
# Генерируем обычный rsa ключ
openssl genrsa -out ca.key 2048 

# Создаем запрос на подпись с именем компонента
openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA/O=system:masters" -out ca.csr

# Подписываем сертификат. В итоге у нас сертификат и ключ, ca.key и ca.crt
openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt
```

##### Генерация сертификата для Администратора кластера:
```bash
# Генерируем обычный rsa ключ
openssl genrsa -out admin.key 2048 

# Создаем запрос на подпись с именем компонента
openssl req -new -key ca.key -subj "/CN=kube-admin" -out admin.csr

# Подписываем сертификат. В итоге у нас сертификат и ключ, ca.key и ca.crt
openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt
```
 Все аналогично созданию рутовых ключей. В отличии от того что мы подписываем сертификат рутовым ключом и сертификатом.

##### Подписание сертификата администратором k8s

>[!info]
>За выполнение операция связанных с сертификатами отвечают два контроллера:
>- CSR-SIGNING
>- CSR-APPROVING

Администратор кластера может подписывать сертификаты пользователей.

Например пользователь создает сертификат и создает запрос на подпись:
- `openssl genrsa -out user.key 2048`
- `openssl -req -new user.key -subj "/CN=user" -out user.csr`

После этого администратор кодирует в base64 csr и создает объект типа `CrtificatesSigningRequest`:
```yaml
apiVersion: certificates.k8s.io/v1 
kind: CertificateSigningRequest 
metadata: 
  name: my-csr 
spec: 
  groups: 
    - system:authenticated 
  request: <Paste the base64 encoded value of the CSR file> 
  signerName: kubernetes.io/kube-apiserver-client 
  usages: 
  - client auth
```

Можно посмотреть весь список данных объектов:
`kubectl get csr`

И апрувнуть если это необходимо:
`kubectl certificate approve my-csr`

Далее ключ можно получить из файла манифеста того же csr, декодировав его.


## Механизмы авторизации

Существует несколько механизмов авторизации в кластер Kubernetes:
- Nodes 
	Данным способом например авторизуется kubelet и другие системные компоненты

- ABAC - Atribute based
	Можно гибко назначить права пользователям. Но для того чтобы добавить пользователю права нужно редактировать единый файл конфигурации и каждый раз перезагружать kubeApiserver, что является неудобным

- RBAC - Roles based
	Стандартный механизм авторизации основанный на ролях создаваемых администратором, которые назначаются определенным пользователям.

- Webhook
	Позволяет использовать сторонние решения для авторизации в кластер. При обращении пользователя к kube-apiserver, кластер обращается к стороннему продукту чтобы узнать у него разрешить ли доступ пользователю или нет.

 - AlwaysAllow 
	 Разрешать все операции.
 
 - AlwaysDeny  
	Ничего не разрешать.

Данные параметры настраиваются в параметрах при запуске kube-apiserver:
```bash
--authorization-mode=AlwaysAllow
```

Так же можно установить несколько параметров:
```bash
--authorization-mode=RBAC,Node,Webhook
```
В этом случае запрос к API авторизуется с использованием каждого механизма в порядке котором вы указали при запуске апи-сервера.

## Role-Based Access Control

Для создания роли нужно создать объект типа `Role`:
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: Developer
rules:
    // Для core группы можно оставить эту строку пустой
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["list", "get", "create", "update", "delete"]
  - apiGroups: [""]
    resources: ["development"]
    verbs: ["list", "get", "update"]
    //Можно ограничинить доступ к определенным объектам с определенным названием
    resourceNames: ["blue", "orange"]
  - apiGroup: [""]
    resources: ["ConfigMap"]
    verbs: ["create"]
```

После создания роли, если мы хотим выдать данную роль например какому нибудь разработчику, нам необходимо создать объект привязки роли - `RoleBinding`:
```yaml
apiVersion:
kind:
metadata:
  name: devusers-developer-binding
  namespace: some_space
roleRef:
    kind: Role
    name: Developer
    apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: User
    name: john
    apiGroup: rbac.authorization.k8s.io

```

##### ClusterRole и ClusterRoleBinding

Аналогичным образом создаются объекты `ClusterRole` и `ClusterRoleBinding`. Отличие только в том что роль кластера распространяется на все неймспейсы.

А так же:
Все ресурсы кластера классифицируются как:
- Namespace - ресурсы принадлежащие кластеру.
	Pods, Deployment, Services, ConfigMaps, Secrets и остальные ресурсы которые создаются в определенном неймспейсе.
	Посмотреть весь список:
		`kubectl api-resources --namespaced=true`
- Cluster scoped - ресурсы области кластера.
	Nodes, PV, ClusterRoles, Namespaces и другие при создании которых мы не указываем namespace, потому-что они работают на уровне выше.
	Посмотреть весь список:
		`kubectl api-resources --namespaced=true

>[!info]
>Можно указывать в `ClusterRole` права на объекты которые работают на уровне `namespace`, соответственно такие права буду распространятся на все `namespace` в кластере.

### ServiceAccounts

В kubernetes есть два типа аккаунтов - пользовательский и сервисный.
Сервисный аккаунт (ServiceAccounts) нужен для доступа сторонних сервисов к API Kubernetes.

Например сервисный аккаунт нужен агенту prometheus для того чтобы отправлять и получать запросы на информацию, для дальнейшей обработки. 

> Для добавления Сервисного аккаунта укажите его в `spec.` пода.
> `spec.serviceAccountName: some-sa` 

ServiceAccount - является объектом k8s. При его создании, автоматически создается токен. Он должен использоваться сторонним приложением для доступа в кластер. Он хранится в виде объекта `Secret`.

Например, чтобы воспользоваться токеном используя команду curl, можно ввести такую команду:
`curl --header "Authorization: Bearer <token>" <url>`

>УСТАРЕЛО В ВЕРСИИ v1.24
	По умолчанию сущеcтвует ServiceAccount `default`. Он автоматически монтируется в каждый под (Если не указан другой), но имеет очень малый набор стандартных разрешений для работы с `KubeAPI` так как в нем токен с неограниченным сроком действия.  Его можно выключить определив это в файле манифеста пода:
	`spec.automountServiceAccountToken: false`

- Автоматическое монтирование токена отключено по умолчанию.
- Чтобы включить монтирование токена в под, требуется явно указать это в манифесте пода или в настройках `ServiceAccount`.
	`spec.serviceAccountName: default`
- Поды могут запросить временный токен для аутентификации с использованием **TokenRequest API**.
Пример создания токена:
`kubectl create token <default> -n <namespace>`

### Безопасность образов

Когда мы запускаем под в тестовой среде мы обычно берем образ  из публичного репозитория, указывая в файле манифеста вот так  `image: nginx`. Когда мы используем изображения на работе, обычно указываем так `image: privat-registry.com/apps/java/some-app:v12.13.3`. Потому-что мы берем образ из частного репозитория, нам необходимо указать полный путь до нашего образа.

Работая с `docker` мы используем команду `docker login` для авторизации в удаленный репозиторий. Но в контексте `kubernetes` нам нужно создать `secret` со встроенным типом `docker-registry` и указать учетные данные:
```bash
kubectl create secret docker-registry java-registry \
--docker-server=registry.io \
--docker-user=admin  \
--docker-password=P@$$w0rd \
--docker-email=admin@mail.com  \
```

Далее необходимо указать данный секрет в спецификации пода:
```
spec:
  containers:
    - name: java-app
      image: registry.io/apps/java-app
  imagePullSecrets:
    - name: java-registry
  
```

### SecurityContext а Pod

Контекст безопасности - это правило для пользователя root, настраиваемое в Поде или в контейнере.

>Если мы определим в спецификации Пода то контекст передастся во все контейнеры.

По умолчанию у пользователя root в контейнере не такие права как у стандартного рута в Linux. Это сделано в целях безопасности. Но мы можем это исправить передав контекст безопасности:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  securityContext:
    runAsUser: 1000         ## Указываем специфичный user-id
    capabilities: 
      add: ["MAC_ADMIN"]    ## Указываем массив с правами которые добавляем
```

### Сетевая безопасность

##### Сетевые политики
Существует два типа траффика:
- Ingress - Входящий трафик;
- Egress - Исходящий трафик.

Допустим нам надо настроить взаимодействие между подами. Под `X` работает на 80 порту а Под `Y` работает на 5000 порту. Мы бы настроили такие правила:
- Pod `X`: 
	- Ingress 80 (Разрешить входящий трафик на 80 порт)
	- Egress 80 (Разрешить исходящий трафик на 80 порту)
- Pod `Y`: 
	- Ingress 5000
	- Egress 5000

По умолчанию в кластере kubernetes у нас стоит правило `All Allow` - которое дает возможность общаться всем контейнерам и подам в кластере без ограничений. Эта возможность есть благодаря [[kube-proxy]].

Пример объекта `NetworkPolicy`:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-policy
spec:
  podSelector:                     # Для какого пода политика
    matchLabels:
      role: app
  policyTypes:
    - Ingress                      # Тип трафика
    - Egress
  ingress:
  - from:
    - podSelector:                 # От какого пода трафик будет идти
        matchLabels:
          name: api-pod
      nameSpaceSelector:           # Указать неймспейс где будет работать политика
        matchLabels:
          name: prod
    - ipBlock:
        cidr: 192.168.1.20/32      # Указать подсеть с которого может поступать
    ports:                         # трафик
    - protocol: TCP
      port: 3306
  
  egress:
  - to:
    - ipBlock:
        cidr: 192.168.1.20/32
    ports:
    - protocol: TCP
      port: 80
  - to:
    - podSelector:
        matchLabels:
          name: api-pod
    ports:
    - protocol: TCP
      port: 80
```

В одном манифесте можно указывать множества правил на входящий и исходящий трафик.
В манифесте выше у нас указано правило на входящий трафик:
>Принимать трафик от пода api-pod который в неймспейсе prod или от пула ip адресов, весь трафик принимать на порт 3306.

Каждый список в списке `from` является отдельным правилом и они разделены логическим `ИЛИ`. А два правило в первом условии разделены логическим `И`

Так же можно разделять логически правило определяя новые блоки `- from` и `- to`
