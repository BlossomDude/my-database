# 1: Kubernetes Shared Volumes

### Problem
```
We are working on an application that will be deployed on multiple containers within a pod on Kubernetes cluster. There is a requirement to share a volume among the containers to save some temporary data. The Nautilus DevOps team is developing a similar template to replicate the scenario. Below you can find more details about it.  
  
1. Create a pod named `volume-share-devops`.  
      
2. For the first container, use image `centos` with `latest` tag only and remember to mention the tag i.e `centos:latest`, container should be named as `volume-container-devops-1`, and run a `sleep` command for it so that it remains in running state. Volume `volume-share` should be mounted at path `/tmp/media`.       
    
3. For the second container, use image `centos` with the `latest` tag only and remember to mention the tag i.e `centos:latest`, container should be named as `volume-container-devops-2`, and again run a `sleep` command for it so that it remains in running state. Volume `volume-share` should be mounted at path `/tmp/cluster`.        
    
4. Volume name should be `volume-share` of type `emptyDir`.       
    
5. After creating the pod, exec into the first container i.e `volume-container-devops-1`, and just for testing create a file `media.txt` with any content under the mounted path of first container i.e `/tmp/media`.       
    
6. The file `media.txt` should be present under the mounted path `/tmp/cluster` on the second container `volume-container-devops-2` as well, since they are using a shared volume.
```

### Solution
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-share-devops
spec:
  containers:
  - name: volume-container-devops-1
    image: centos:latest
    command: ["sleep", "1000"]
    volumeMounts:
    - mountPath: /tmp/media
      name: volume-share
  - name: volume-container-devops-2
    image: centos:latest
    command: ["sleep", "1000"]
    volumeMounts:
    - mountPath: /tmp/cluster
      name: volume-share
  volumes:
    - name: volume-share
      emptyDir:
```

``` bash
kubectl apply -f values.yaml
kubectl exec -it volume-share-devops -c volume-container-devops-1 sh
	echo "hello" > /tmp/media/media.txt
kubectl exec -it volume-share-devops -c volume-container-devops-2 sh
	cat /tmp/cluster/media.txt


```

# 2: Kubernetes Sidecar Containers

### Problem
```
We have a web server container running the nginx image. The access and error logs generated by the web server are not critical enough to be placed on a persistent volume. However, Nautilus developers need access to the last 24 hours of logs so that they can trace issues and bugs. Therefore, we need to ship the access and error logs for the web server to a log-aggregation service. Following the separation of concerns principle, we implement the Sidecar pattern by deploying a second container that ships the error and access logs from nginx. Nginx does one thing, and it does it well—serving web pages. The second container also specializes in its task—shipping logs. Since containers are running on the same Pod, we can use a shared emptyDir volume to read and write logs.

1. Create a pod named `webserver`.
    
2. Create an `emptyDir` volume `shared-logs`.
    
3. Create two containers from `nginx` and `ubuntu` images with `latest` tag only and remember to mention tag i.e `nginx:latest`, nginx container name should be `nginx-container` and ubuntu container name should be `sidecar-container` on webserver pod.
    
4. Add command on sidecar-container `"sh","-c","while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done"`
    
5. Mount the volume `shared-logs` on both containers at location `/var/log/nginx`, all containers should be up and running.
```

### Solution
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webserver
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    volumeMounts:
    - mountPath: /var/log/nginx
      name: shared-logs

  - name: sidecar-container
    image: ubuntu:latest
    command: ["sh","-c","while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done"]
    volumeMounts:
    - mountPath: /var/log/nginx
      name: shared-logs

  volumes:
  - name: shared-logs
    emptyDir:
```



# 3: Setup Kubernetes Namespaces and PODs

### Problem
```
Some of the Nautilus team developers are developing a static website and they want to deploy it on Kubernetes cluster. They want it to be highly available and scalable. Therefore, based on the requirements, the DevOps team has decided to create a deployment for it with multiple replicas. Below you can find more details about it:
 
1. Create a deployment using `nginx` image with `latest` tag only and remember to mention the tag i.e `nginx:latest`. Name it as `nginx-deployment`. The container should be named as `nginx-container`, also make sure replica counts are `3`.
    
2. Create a `NodePort` type service named `nginx-service`. The nodePort should be `30011`.
```

### Solution
```bash
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx-deployment
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx:latest

---

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30011
  selector:
    app: nginx
```



# 4: Print Environment Variables

### Problem
```
The Nautilus DevOps team is working on to setup some pre-requisites for an application that will send the greetings to different users. There is a sample deployment, that needs to be tested. Below is a scenario which needs to be configured on Kubernetes cluster. Please find below more details about it.

1. Create a `pod` named `print-envars-greeting`.
    
2. Configure spec as, the container name should be `print-env-container` and use `bash` image.
    
3. Create three environment variables:
    
a. `GREETING` and its value should be `Welcome to`
b. `COMPANY` and its value should be `DevOps`
c. `GROUP` and its value should be `Datacenter`

4. Use command `["/bin/sh", "-c", 'echo "$(GREETING) $(COMPANY) $(GROUP)"']` (please use this exact command), also set its `restartPolicy` policy to `Never` to avoid crash loop back.
5. 
6. You can check the output using `kubectl logs -f print-envars-greeting` command. 
```

### Solution
```yaml
apiVersion: v1
kund: Pod
metadata:
  name: print-envars-greeting
spec:
  containers:
  - name: print-env-container
    image: bash:latest
    command: ["/bin/sh", "-c", 'echo "$(GREETING) $(COMPANY) $(GROUP)"']
    restartPolicy: Never
    env:
    - name: GREETING
      value: "Welcome to"
    - name: COMPANY
      value: "DevOps"
    - name: GROUP
      value: "Datacenter"
```



# 5: Execute Rolling Updates in Kubernetes

### Problem
```
An application currently running on the Kubernetes cluster employs the nginx web server. The Nautilus application development team has introduced some recent changes that need deployment. They've crafted an image `nginx:1.17` with the latest updates.

Execute a rolling update for this application, integrating the `nginx:1.17` image. The deployment is named `nginx-deployment`.

Ensure all pods are operational post-update.

`Note:` The `kubectl` utility on `jump_host` is set up to operate with the Kubernetes cluster

---

Приложение, которое в настоящее время выполняется в кластере Kubernetes, использует веб-сервер nginx. Команда разработчиков приложений Nautilus внесла некоторые изменения, которые требуют развертывания. Они создали образ nginx:1.17 с последними обновлениями.  
  
Выполните текущее обновление для этого приложения, интегрируя образ nginx:1.17. Развертывание называется nginx-deployment.  
  
Убедитесь, что все модули работают после обновления.  
  
Примечание: Утилита kubectl на jump_host настроена для работы с кластером Kubernetes
```

### Solution
```bash
kubectl set image deployment/nginx-deployment nginx-container=nginx:1.17
```



# 6: Revert Deployment to Previous Version in Kubernetes

### Problem
```
Earlier today, the Nautilus DevOps team deployed a new release for an application. However, a customer has reported a bug related to this recent release. Consequently, the team aims to revert to the previous version.

There exists a deployment named `nginx-deployment`; initiate a rollback to the previous revision.

`Note:` The `kubectl` utility on `jump_host` is configured to interact with the Kubernetes cluster.

---

Ранее сегодня команда разработчиков Nautilus выпустила новую версию приложения. Однако клиент сообщил об ошибке, связанной с этим недавним выпуском. Следовательно, команда намерена вернуться к предыдущей версии.  
  
Существует развертывание с именем nginx-deployment; инициируйте откат к предыдущей версии.  

Примечание: Утилита kubectl на jump_host настроена для взаимодействия с кластером Kubernetes.
```

### Solution
```bash
kubectl get deployment
kubectl rollout undo deployment/nginx-deployment
```



# 7: Deploy Replica Set in Kubernetes Cluster

### Problem
```
The Nautilus DevOps team is gearing up to deploy applications on a Kubernetes cluster for migration purposes. A team member has been tasked with creating a ReplicaSet outlined below:  
  
1. Create a ReplicaSet using `httpd` image with `latest` tag (ensure to specify as `httpd:latest`) and name it `httpd-replicaset`.          
2. Apply labels: `app` as `httpd_app`, `type` as `front-end`.    
3. Name the container `httpd-container`. Ensure the replica count is `4`.

---

Команда Nautilus DevOps готовится к развертыванию приложений в кластере Kubernetes для целей миграции. Перед членом команды была поставлена задача создать набор реплик, описанный ниже:  
   
1)Создайте набор реплик, используя httpd image с тегом latest (обязательно укажите как httpd:latest) и назовите его httpd-replicaset.    
2)Примените ярлыки: app - как httpd_app, введите как front-end.   
3)Назовите контейнер httpd-container. Убедитесь, что количество реплик равно 4.
```

### Solution
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: httpd-replicaset
  labels:
    app: httpd_app
    type: front-end
spec:
  replicas: 4
  selector:
    matchLabels:
      rsget: yos
  template:
    metadata:
      labels:
        rsget: yos
    spec:
      containers:
        - name: httpd-container
          image: httpd:latest
```



# 8: Schedule Cronjobs in Kubernetes

### Problem
```
The Nautilus DevOps team is setting up recurring tasks on different schedules. Currently, they're developing scripts to be executed periodically. To kickstart the process, they're creating cron jobs in the Kubernetes cluster with placeholder commands. Follow the instructions below:  
  
1. Create a cronjob named `nautilus`.        
2. Set Its schedule to something like `*/2 * * * *`. You can set any schedule for now.            
3. Name the container `cron-nautilus`.        
4. Utilize the `nginx` image with `latest tag` (specify as `nginx:latest`).        
5. Execute the dummy command `echo Welcome to xfusioncorp!`.           
6. Ensure the restart policy is `OnFailure`.

---

Команда разработчиков Nautilus настраивает повторяющиеся задачи по разным расписаниям. В настоящее время они разрабатывают сценарии для периодического выполнения. Чтобы запустить процесс, они создают cron-задания в кластере Kubernetes с помощью команд-заполнителей. Следуйте приведенным ниже инструкциям.:

1)Создайте cronjob с именем nautilus.
2)Установите для него расписание примерно на */2 * * * *. На данный момент вы можете установить любое расписание.
3)Назовите контейнер cron-nautilus .
4)Используйте образ nginx с тегом latest (укажите как nginx: latest).
5)Выполните фиктивную команду echo Добро пожаловать в xfusioncorp!.
6)Убедитесь, что политика перезапуска выполнена по ошибке.

```

### Solution
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: nautilus
spec:
  schedule: "*/2 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: cron-nautilus
              image: nginx:latest
              args:
                - /bin/sh
                - -c
                - echo "Welcome to xfusioncorp!"
          restartPolicy: OnFailure
```



# 9: Create Countdown Job in Kubernetes

### Problem
```
The Nautilus DevOps team is crafting jobs in the Kubernetes cluster. While they're developing actual scripts/commands, they're currently setting up templates and testing jobs with dummy commands. Please create a job template as per details given below:

1. Create a job named `countdown-datacenter`. 
2. The spec template should be named `countdown-datacenter` (under metadata), and the container should be named `container-countdown-datacenter`  
3. Utilize image `centos` with `latest` tag (ensure to specify as `centos:latest`), and set the restart policy to `Never`.   
4. Execute the command `sleep 5`

---
Команда Nautilus DevOps разрабатывает задания в кластере Kubernetes. Разрабатывая реальные сценарии/команды, они в настоящее время настраивают шаблоны и тестируют задания с фиктивными командами. Пожалуйста, создайте шаблон задания в соответствии с приведенными ниже подробностями: 

- Создайте задание с именем countdown-data center. 
- Шаблон спецификации должен называться countdown-datacenter (в разделе "метаданные"), а контейнер должен называться container-countdown-datacenter 
- Используйте centos-образ с тегом latest (обязательно укажите как centos:latest) и установите для политики перезапуска значение Never. 
- Выполните команду sleep 5

```

### Solution
```bash
apiVersion: batch/v1
kind: Job
metadata:
  name: nautilus
spec:
  template:
    metadata:
      name: spec-name
    spec:
	  containers:
		- name: cron-nautilus
		  image: centos:latest
		  args:
			- /bin/sh
			- -c
			- sleep 5
	  restartPolicy: Never
```



# 10: Set Up Time Check Pod in Kubernetes

### Problem
``` txt
The Nautilus DevOps team needs a time check pod created in a specific Kubernetes namespace for logging purposes. Initially, it's for testing, but it may be integrated into an existing cluster later. Here's what's required:

1. Create a pod called `time-check` in the `nautilus` namespace. The pod should contain a container named `time-check`, utilizing the `busybox` image with the `latest` tag (specify as `busybox:latest`).   
2. Create a config map named `time-config` with the data `TIME_FREQ=10` in the same namespace.   
3. Configure the `time-check` container to execute the command: `while true; do date; sleep $TIME_FREQ;done`. Ensure the result is written `/opt/data/time/time-check.log`. Also, add an environmental variable `TIME_FREQ` in the container, fetching its value from the config map `TIME_FREQ` key.    
4. Create a volume `log-volume` and mount it at `/opt/data/time` within the container.

---

Команде разработчиков Nautilus нужен модуль проверки времени, созданный в определенном пространстве имен Kubernetes для ведения журнала. Изначально он предназначен для тестирования, но позже может быть интегрирован в существующий кластер. Вот что требуется для этого.:

1. Создайте модуль с именем time-check в пространстве имен nautilus. Модуль должен содержать контейнер с именем time-check, использующий изображение busybox с тегом latest (укажите как busybox:latest).
2. Создайте карту конфигурации с именем time-config с данными TIME_FREQ= 10 в том же пространстве имен.
3. Настройте контейнер проверки времени для выполнения команды: while true; do date; sleep $TIME_FREQ;готово. Убедитесь, что результат записан /opt/data/time/time-check.log. Также добавьте переменную среды TIME_FREQ в контейнер, извлекая ее значение из ключа TIME_FREQ конфигурационной карты.
4. Создайте журнал тома-volume и смонтируйте его в /opt/data / time внутри контейнера.
```

### Solution
```bash
apiVersion: v1
kind: ConfigMap
metadata:
  name: time-config
  namespace: devops
data:
  TIME_FREQ: "3"

---

apiVersion: v1
kind: Pod
metadata:
  name: time-check
  namespace: devops
spec:
  volumes:
    - name: log-volume
      emptyDir: {}
  containers:
    - name: time-check
      image: busybox:latest
      volumeMounts:
        - mountPath: /opt/devops/time
          name: log-volume
      envFrom:
        - configMapRef:
            name: time-config
      command: ["/bin/sh", "-c"]
      args:
        [
        "while true; do date; sleep $TIME_FREQ;done > /opt/devops/time/time-check.log",
        ]



```


# 11: Resolve Pod Deployment Issue

### Problem
```
A junior DevOps team member encountered difficulties deploying a stack on the Kubernetes cluster. The pod fails to start, presenting errors. Let's troubleshoot and rectify the issue promptly.

  
1. There is a pod named `webserver`, and the container within it is named `httpd-container`, its utilizing the `httpd:latest` image.
    
2. Additionally, there's a sidecar container named `sidecar-container` using the `ubuntu:latest` image.
    

Identify and address the issue to ensure the pod is in the `running` state and the application is accessible.

`Note:` The `kubectl` utility on `ju`

---

Младший сотрудник команды DevOps столкнулся с трудностями при развертывании стека в кластере Kubernetes. Модуль не запускается, что приводит к появлению ошибок. Давайте быстро разберемся с неполадками и устраним проблему.  
  
Существует модуль с именем webserver, а контейнер внутри него называется httpd-container, в котором используется образ httpd:latest.  
  
Кроме того, есть контейнер sidecar с именем sidecar-container, использующий ubuntu:latest image.  
  
Определите и устраните проблему, чтобы убедиться, что модуль находится в запущенном состоянии и приложение доступно.
```

### Solution
```bash
kubectl get pods -o yaml > pod.yaml
```



# 12: Update Deployment and Service in Kubernetes

### Problem
```
An application deployed on the Kubernetes cluster requires an update with new features developed by the Nautilus application development team. The existing setup includes a deployment named `nginx-deployment` and a service named `nginx-service`. Below are the necessary changes to be implemented without deleting the deployment and service:

1.) Modify the service nodeport from `30008` to `32165`
2.) Change the replicas count from `1` to `5`
3.) Update the image from `nginx:1.18` to `nginx:latest`

---

Приложению, развернутому в кластере Kubernetes, требуется обновление с новыми функциями, разработанными командой разработчиков приложений Nautilus. Существующая установка включает в себя развертывание с именем nginx-deployment и службу с именем nginx-service. Ниже приведены необходимые изменения, которые необходимо внести без удаления развертывания и службы.:  
    
1.) Измените порт служебного узла с 30008 на 32165   
2.) Измените количество реплик с 1 на 5
3.) Обновите изображение с nginx:1.18 на nginx:latest
```

### Solution
```bash
#1
kubectl edit svc nginx-service
#2
kubectl edit deploy nginx-deployment
#3
kubectl set image deployment/nginx-deployment nginx-container=nginx:latest
```



# 13: Deploy Highly Available Pods with Replication Controller

### Problem
```
The Nautilus DevOps team is establishing a `ReplicationController` to deploy multiple pods for hosting applications that require a highly available infrastructure. Follow the specifications below to create the `ReplicationController`:

1. Create a `ReplicationController` using the `httpd` image, preferably with `latest` tag, and name it `httpd-replicationcontroller`.
    
2. Assign labels `app` as `httpd_app`, and `type` as `front-end`. Ensure the container is named `httpd-container` and set the replica count to `3`.  

All `pods` should be running state post-deployment.  

---

Команда разработчиков Nautilus разрабатывает ReplicationController для развертывания нескольких модулей для размещения приложений, которым требуется высокодоступная инфраструктура. Чтобы создать ReplicationController, следуйте приведенным ниже спецификациям:  
  
Создайте ReplicationController, используя httpd-образ, желательно с тегом latest, и назовите его httpd-replicationcontroller.  
  
Присвойте приложению labels имя httpd_app и введите как front-end. Убедитесь, что контейнер называется httpd-container, и установите число реплик равным 3.  
  
Все модули должны быть запущены после развертывания.
```

### Solution
```bash
apiVersion: v1
kind: ReplicationController
metadata:
  name: httpd-replicationcontroller
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: httpd_app
        type: front-end
    spec:
      containers:
        - name: httpd-container
          image: httpd:latest
```



# 14: Resolve Volume Mounts Issue in Kubernetes

### Problem
```
We encountered an issue with our Nginx and PHP-FPM setup on the Kubernetes cluster this morning, which halted its functionality. Investigate and rectify the issue:  

The pod name is `nginx-phpfpm` and configmap name is `nginx-config`. Identify and fix the problem.  

Once resolved, copy `/home/thor/index.php` file from the `jump host` to the `nginx-container` within the nginx document root. After this, you should be able to access the website using `Website` button on the top bar.

---

Сегодня утром мы столкнулись с проблемой при настройке Nginx и PHP-FPM в кластере Kubernetes, которая привела к остановке его работы. Изучите и устраните проблему:  
  
Имя модуля - nginx-phpfpm, а имя configmap - nginx-config. Определите и устраните проблему.  
  
После устранения скопируйте файл /home/thor/index.php с хоста jump в nginx-контейнер в корневом каталоге документов nginx. После этого вы сможете получить доступ к веб-сайту, используя кнопку "Веб-сайт" на верхней панели.
```

### Solution
```bash

```
