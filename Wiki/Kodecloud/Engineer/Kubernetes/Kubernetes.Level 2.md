# 1: Kubernetes Shared Volumes

### Problem
```
We are working on an application that will be deployed on multiple containers within a pod on Kubernetes cluster. There is a requirement to share a volume among the containers to save some temporary data. The Nautilus DevOps team is developing a similar template to replicate the scenario. Below you can find more details about it.  
  
1. Create a pod named `volume-share-devops`.  
      
2. For the first container, use image `centos` with `latest` tag only and remember to mention the tag i.e `centos:latest`, container should be named as `volume-container-devops-1`, and run a `sleep` command for it so that it remains in running state. Volume `volume-share` should be mounted at path `/tmp/media`.       
    
3. For the second container, use image `centos` with the `latest` tag only and remember to mention the tag i.e `centos:latest`, container should be named as `volume-container-devops-2`, and again run a `sleep` command for it so that it remains in running state. Volume `volume-share` should be mounted at path `/tmp/cluster`.        
    
4. Volume name should be `volume-share` of type `emptyDir`.       
    
5. After creating the pod, exec into the first container i.e `volume-container-devops-1`, and just for testing create a file `media.txt` with any content under the mounted path of first container i.e `/tmp/media`.       
    
6. The file `media.txt` should be present under the mounted path `/tmp/cluster` on the second container `volume-container-devops-2` as well, since they are using a shared volume.
```

### Solution
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-share-devops
spec:
  containers:
  - name: volume-container-devops-1
    image: centos:latest
    command: ["sleep", "1000"]
    volumeMounts:
    - mountPath: /tmp/media
      name: volume-share
  - name: volume-container-devops-2
    image: centos:latest
    command: ["sleep", "1000"]
    volumeMounts:
    - mountPath: /tmp/cluster
      name: volume-share
  volumes:
    - name: volume-share
      emptyDir:
```

``` bash
kubectl apply -f values.yaml
kubectl exec -it volume-share-devops -c volume-container-devops-1 sh
	echo "hello" > /tmp/media/media.txt
kubectl exec -it volume-share-devops -c volume-container-devops-2 sh
	cat /tmp/cluster/media.txt


```

# 2: Kubernetes Sidecar Containers

### Problem
```
We have a web server container running the nginx image. The access and error logs generated by the web server are not critical enough to be placed on a persistent volume. However, Nautilus developers need access to the last 24 hours of logs so that they can trace issues and bugs. Therefore, we need to ship the access and error logs for the web server to a log-aggregation service. Following the separation of concerns principle, we implement the Sidecar pattern by deploying a second container that ships the error and access logs from nginx. Nginx does one thing, and it does it well—serving web pages. The second container also specializes in its task—shipping logs. Since containers are running on the same Pod, we can use a shared emptyDir volume to read and write logs.

1. Create a pod named `webserver`.
    
2. Create an `emptyDir` volume `shared-logs`.
    
3. Create two containers from `nginx` and `ubuntu` images with `latest` tag only and remember to mention tag i.e `nginx:latest`, nginx container name should be `nginx-container` and ubuntu container name should be `sidecar-container` on webserver pod.
    
4. Add command on sidecar-container `"sh","-c","while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done"`
    
5. Mount the volume `shared-logs` on both containers at location `/var/log/nginx`, all containers should be up and running.
```

### Solution
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: webserver
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    volumeMounts:
    - mountPath: /var/log/nginx
      name: shared-logs

  - name: sidecar-container
    image: ubuntu:latest
    command: ["sh","-c","while true; do cat /var/log/nginx/access.log /var/log/nginx/error.log; sleep 30; done"]
    volumeMounts:
    - mountPath: /var/log/nginx
      name: shared-logs

  volumes:
  - name: shared-logs
    emptyDir:
```



# 3: Setup Kubernetes Namespaces and PODs

### Problem
```
Some of the Nautilus team developers are developing a static website and they want to deploy it on Kubernetes cluster. They want it to be highly available and scalable. Therefore, based on the requirements, the DevOps team has decided to create a deployment for it with multiple replicas. Below you can find more details about it:
 
1. Create a deployment using `nginx` image with `latest` tag only and remember to mention the tag i.e `nginx:latest`. Name it as `nginx-deployment`. The container should be named as `nginx-container`, also make sure replica counts are `3`.
    
2. Create a `NodePort` type service named `nginx-service`. The nodePort should be `30011`.
```

### Solution
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginx-deployment
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx:latest

---

apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30011
  selector:
    app: nginx
```



# 4: Print Environment Variables

### Problem
```
The Nautilus DevOps team is working on to setup some pre-requisites for an application that will send the greetings to different users. There is a sample deployment, that needs to be tested. Below is a scenario which needs to be configured on Kubernetes cluster. Please find below more details about it.

1. Create a `pod` named `print-envars-greeting`.
    
2. Configure spec as, the container name should be `print-env-container` and use `bash` image.
    
3. Create three environment variables:
    
a. `GREETING` and its value should be `Welcome to`
b. `COMPANY` and its value should be `DevOps`
c. `GROUP` and its value should be `Datacenter`

4. Use command `["/bin/sh", "-c", 'echo "$(GREETING) $(COMPANY) $(GROUP)"']` (please use this exact command), also set its `restartPolicy` policy to `Never` to avoid crash loop back.
5. 
6. You can check the output using `kubectl logs -f print-envars-greeting` command. 
```

### Solution
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: print-envars-greeting
spec:
  containers:
  restartPolicy: Never
  - name: print-env-container
    image: bash:latest
    command: ["/bin/sh", "-c", 'echo "$(GREETING) $(COMPANY) $(GROUP)"']
    env:
    - name: GREETING
      value: "Welcome to"
    - name: COMPANY
      value: "DevOps"
    - name: GROUP
      value: "Datacenter"
```



# 5: Rolling Updates And Rolling Back Deployments in Kubernetes

### Problem
```
There is a production deployment planned for next week. The Nautilus DevOps team wants to test the deployment update and rollback on Dev environment first so that they can identify the risks in advance. Below you can find more details about the plan they want to execute.  
  
1. Create a namespace `devops`. Create a deployment called `httpd-deploy` under this new namespace, It should have one container called `httpd`, use `httpd:2.4.25` image and `2` replicas. The deployment should use `RollingUpdate` strategy with `maxSurge=1`, and `maxUnavailable=2`. Also create a `NodePort` type service named `httpd-service` and expose the deployment on `nodePort: 30008`.  
        
2. Now upgrade the deployment to version `httpd:2.4.43` using a rolling update.  
        
3. Finally, once all pods are updated undo the recent update and roll back to the previous/original version.
```

### Solution
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-deploy
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 2
  selector:
    matchLabels:
      app: httpd
  template:
    metadata:
      name: httpd-template
      labels:
        app: httpd
    spec:
      containers:
      - name: httpd
        image: httpd:2.4.25

---

apiVersion: v1
kind: Service
metadata:
  name: httpd-service
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30008
  selector:
    app: httpd
```

```bash 
vi values.yaml
kubectl create ns devops
kubectl config set-context --current --namespace=devops
kubectl set image deployment httpd-deploy httpd=httpd:2.4.43 --record=true
kubectl rollout undo deployment/httpd-deploy
```



# 6: Deploy Jenkins on Kubernetes

### Problem
```
The Nautilus DevOps team is planning to set up a Jenkins CI server to create/manage some deployment pipelines for some of the projects. They want to set up the Jenkins server on Kubernetes cluster. Below you can find more details about the task:

1) Create a namespace `jenkins`

2) Create a Service for jenkins deployment. Service name should be `jenkins-service` under `jenkins` namespace, type should be `NodePort`, nodePort should be `30008`

3) Create a Jenkins Deployment under `jenkins` namespace, It should be name as `jenkins-deployment` , labels `app` should be `jenkins` , container name should be `jenkins-container` , use `jenkins/jenkins` image , containerPort should be `8080` and replicas count should be `1`.  
  
Make sure to wait for the pods to be in running state and make sure you are able to access the Jenkins login screen in the browser before hitting the `Check` button.
```

### Solution
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkins-deployment
  labels:
    app: jenkins
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jenkins
  template:
    metadata:
      name: jenkins-tpl
      labels:
        app: jenkins
    spec:
      containers:
      - name: jenkins-container
        image: jenkins/jenkins

---

apiVersion: v1
kind: Service
metadata:
  name: jenkins-service
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30008
    targetPort: 8080
  selector:
    app: jenkins
```



# 7: Deploy Grafana on Kubernetes Cluster

### Problem
```
The Nautilus DevOps teams is planning to set up a Grafana tool to collect and analyze analytics from some applications. They are planning to deploy it on Kubernetes cluster. Below you can find more details.   

1.) Create a deployment named `grafana-deployment-devops` using any grafana image for Grafana app. Set other parameters as per your choice.  
  
2.) Create `NodePort` type service with nodePort `32000` to expose the app.  
  
`You need not to make any configuration changes inside the Grafana app once deployed, just make sure you are able to access the Grafana login page.`
```

### Solution
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana-deployment-devops
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      name: tpl-grafana
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana-container
        image: grafana/grafana:latest
        ports:
          - containerPort: 3000

---

apiVersion: v1
kind: Service
metadata:
  name: grafana-service
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 32000
    targetPort: 3000
  selector:
    app: grafana
```



# 8: Schedule Cronjobs in Kubernetes

### Problem
```
A new java-based application is ready to be deployed on a Kubernetes cluster. The development team had a meeting with the DevOps team to share the requirements and application scope. The team is ready to setup an application stack for it under their existing cluster. Below you can find the details for this:

1. Create a namespace named `tomcat-namespace-nautilus`.
    
2. Create a `deployment` for tomcat app which should be named as `tomcat-deployment-nautilus` under the same namespace you created. Replica count should be `1`, the container should be named as `tomcat-container-nautilus`, its image should be `gcr.io/kodekloud/centos-ssh-enabled:tomcat` and its container port should be `8080`.
    
3. Create a `service` for tomcat app which should be named as `tomcat-service-nautilus` under the same namespace you created. Service type should be `NodePort` and nodePort should be `32227`.  
      
Before clicking on `Check` button please make sure the application is up and running.  
  
`You can use any labels as per your choice.`

```

### Solution
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deployment-nautilus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tomcat
  template:
    metadata:
      labels:
        app: tomcat
    spec:
      containers:
      - name: tomcat-container-nautilus
        image: gcr.io/kodekloud/centos-ssh-enabled:tomcat
        ports:
          - containerPort: 8080

---

apiVersion: v1
kind: Service
metadata:
  name: tomcat-service-nautilus
spec:
  type: NodePort
  ports:
  - port: 8080
    nodePort: 32227
    targetPort: 8080
  selector:
    app: tomcat
```



# 9: Deploy Node App on Kubernetes

### Problem
```
The Nautilus development team has completed development of one of the node applications, which they are planning to deploy on a Kubernetes cluster. They recently had a meeting with the DevOps team to share their requirements. Based on that, the DevOps team has listed out the exact requirements to deploy the app. Find below more details:

1. Create a deployment using `gcr.io/kodekloud/centos-ssh-enabled:node` image, replica count must be `2`.
    
2. Create a service to expose this app, the service type must be `NodePort`, targetPort must be `8080` and nodePort should be `30012`.
    
3. Make sure all the pods are in `Running` state after the deployment.
    
4. You can check the application by clicking on `NodeApp` button on top bar.

```

### Solution
```yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: centos-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: centos
  template:
    metadata:
      labels:
        app: centos
    spec:
      containers:
      - name: centos-container
        image: gcr.io/kodekloud/centos-ssh-enabled:node
        ports:
          - containerPort: 8080

---

apiVersion: v1
kind: Service
metadata:
  name: tomcat-service-nautilus
spec:
  type: NodePort
  ports:
  - port: 8080
    nodePort: 30012
    targetPort: 8080
  selector:
    app: centos
```



# 10: Set Up Time Check Pod in Kubernetes

### Problem
``` txt
Last week, the Nautilus DevOps team deployed a redis app on Kubernetes cluster, which was working fine so far. This morning one of the team members was making some changes in this existing setup, but he made some mistakes and the app went down. We need to fix this as soon as possible. Please take a look.  

The deployment name is `redis-deployment`. The pods are not in running state right now, so please look into the issue and fix the same.  

`Note:` The `kubectl` utility on `jump_host` has been configured to work with the kubernetes cluster.
```

### Solution
```yaml
# Fix syntax mistakes 
image: redis:alpine
...
volume:
...
  redis-config
```


# 11: 

### Problem
```
We deployed an Nginx and PHPFPM based application on Kubernetes cluster last week and it had been working fine. This morning one of the team members was troubleshooting an issue with this stack and he was supposed to run Nginx welcome page for now on this stack till issue with phpfpm is fixed but he made a change somewhere which caused some issue and the application stopped working. Please look into the issue and fix the same:

The deployment name is nginx-phpfpm-dp and service name is nginx-service. Figure out the issues and fix them. FYI Nginx is configured to use default http port, node port is 30008 and copy index.php under /tmp/index.php to deployment under /var/www/html. Please do not try to delete/modify any other existing components like deployment name, service name etc.

Note: The kubectl utility on jump_host has been configured to work with the kubernetes cluster.
```

### Solution
```bash
https://github.com/joseeden/KodeKloud_Engineer_Labs/blob/main/Tasks_Kubernetes/Level_2/Lab_011_Fix_issue_with_LAMP_Environment_in_Kubernetes.md
```
