Важная составляющая нашего кластера это сеть. Без сети мы бы не смогли отправлять запросы к kube-apiserver, мы не могли бы получить доступ к подам и так далее.

Каждый узел, в том числе мастер - должен иметь минимум один настроенный сетевой интерфейс для общения, а так же порты для общения наших компонентов.


| Component               | port        |
| ----------------------- | ----------- |
| kube-apiserver          | 6443        |
| kube-scheduler          | 10259       |
| kube-controller-manager | 10257       |
| etcd                    | 2379, 2380  |
| kubelet                 | 10250       |
| worker-nodes            | 30000-32767 |
[google.com](https://google.com)
## CNI 
Плагины CNI хранятся на мастер узле в директории:
`/opt/bin/cni/`

Конфигурация плагинов находится в директории:
`/etc/cni/net.d/`
Файл конфигурации выбирается в алфавитном порядке.

Пример файла конфигурации:
```json
{
    "cniVersion": "1.1.0",
    "name": "dbnet",
    "type": "bridge",
    "bridge": "cni0",
    "keyA": ["some more", "plugin specific", "configuration"],
    "ipam": {
        "type": "host-local",
        "subnet": "10.1.0.0/16",
        "gateway": "10.1.0.1"
    },
    "dns": {
        "nameservers": [ "10.1.0.1" ]
    }
}
```

- `cniVersion: "1.1.0"` — указывает версию спецификации CNI, используемую в этой конфигурации. Важно для обеспечения совместимости между CNI-плагинами и инфраструктурой.
- `name: dbnet` — уникальное имя сети, которое будет использоваться для идентификации.
    - Это имя помогает различать разные сети в одной системе.
- `type: "bridge"` — тип плагина. Указывает, что сеть будет создаваться с использованием мостового интерфейса.
- `bridge: "cni0"` — имя мостового интерфейса на хосте, который создаст плагин.
    - Если интерфейс `cni0` уже существует, плагин будет использовать его.
- `keyA: ["some more", "plugin specific", "configuration"]` — дополнительная, специфичная для плагина конфигурация.
    - Она позволяет передавать кастомные параметры, которые может обрабатывать только данный плагин.
- **`ipam`** (IP Address Management):
    - Настройки управления IP-адресами для этой сети.
        - **`type`**: `"host-local"` — локальное управление IP-адресами, где адреса хранятся на хосте.
        - **`subnet`**: `"10.1.0.0/16"` — диапазон IP-адресов, доступный для выделения контейнерам.
        - **`gateway`**: `"10.1.0.1"` — IP-адрес шлюза для этой сети. Все внешние пакеты будут проходить через этот адрес.
- `dns: nameservers`:
        - `["10.1.0.1"]` — адрес DNS-сервера, который будет использоваться контейнерами для разрешения доменных имен.

##### Weave
- На каждом узле назначает своего агента
- По умолчанию выдает ip в диапазоне 10.32.0.0/12

## CoreDNS

По умолчанию в кластере работает встроенный CoreDNS.

- При создании [[Service]] - создается DNS-запись. Таким образом дает возможность всем подам в кластере достучаться до пода с сервисом.
- Если мы обратимся от одного пода к другому в рамках одного неймспейса то мы можем обратится по имени сервиса так: `curl http://web-server`. Но если под находится в другом неймспейсе, то это нужно указать явно: `curl http://web-service.blue`
Пример таблицы DNS:

| hostname    | namespace | type | root          | IP            |
| ----------- | --------- | ---- | ------------- | ------------- |
| web-service | blue      | svc  | cluster.local | 10.107.37.188 |
| 10-244-4-5  | blue      | pod  | cluster.local | 10.244.4.5    |
Для пода автоматически назначается имя - ip адрес с заменой точек на дефисы.

`CoreDNS` - является сервером dns по умолчанию начиная с версии 1.12. Он работает как под (Он находится в deployment и replica set).

Для работы CoreDNS требуется файл конфигурации. Он находится в `/etc/coredns/Corefile` И передается в под CoreDNS в виде [[ConfigMap & Secret|ConfigMap]]:
```
.:53 {
	errors
	health
	kubernetes cluster.local in-addr.arpa ip6.arpa {
		pods insecure
		updtream
		falltrough in-addr.arpa ip6.arpa
	}
	prometheus :9153
	proxy . /etc/resolv.conf
	cache 30
	reload
}
```
Он содержит плагины для обработки ошибок, мониторинга, кэширования и тд.
Для работы с k8s он содержит соответствующий плагин - kubernetes.
- В данном примере `cluster.local` указан как доменное имя верхнего уровня. таким образом каждая запись на сервере CoreDNS попадает под этот домен.
- `pods insecure` - включает преобразования  ip пода в hostname заменяя точки на дефисы.
>[! info] 
>Плагин `proxy` в файле конфигурации:
>Любая запись которую DNS сервер не сможет разрешить, например если под решил достучаться до гугла. Он перенаправляется на `nameserver` в файле `/etc/resolv.conf` находящийся в контейнере самого CoreDNS 

При запуске CoreDNS создается [[Service]] kube-dns - Ip-адрес которого является dns-сервером по умолчанию для каждого созданного пода в клaстере. За это отвечает [[kubelet]]

## Ingress Controller

Ingress Controller - решает проблему управления входящем трафиком, направляя его к нужным сервисам. А так же:
- Маршрутизация трафика
- Балансировка нагрузки
- Терминация TLS/SSL. 
	Снимает нагрузку по шифрованию и дешифрованию с сервисом приложений.
- Мониторинг и логирование

Существует много разных сторонних решений. Например nginx, GCE, traefik, HAproxy etc. (Первые два поддерживаются проектом kubernetes.) `Ingress Controller` не поставляется по умолчанию с кластером `kubernetes`.

##### Настройка Ingress Controller
Ingress Controller разворачивается в Deployment.
На примере nginx:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-ingress
  template:
    metadata;
      labels:
        app: nginx-ingress
    spec:
      containers:
        - name: nginx-ingress-controller
          image: <nginx-controller>
          
		  # Запуск ingress и добавление файла конфигурации из CM
		  args: 
		    - /nginx-ingress-controller
		    - --configmap=$(POD_NAMESPACE)/nginx-configuration
		  
		  ## Нужно передать две переменные для работы nginx
		  env:
		    - name: POD_NAME
		      valueFrom:
		        fieldRef:
		          fieldPath: metadata.name
		    - name: POD_NAMESPACE
		      valueFrom:
		        fieldRef:
		          fieldPath: metadata.namespace
		  
		  ports:
		    - name: http
		      containerPort: 80
		    - name: https:
		      containerPort: 443
```

Так же необходим Config Map Для написания в него конфигурации nginx:
```yaml
apiVersion: v1
kind: Configmap
metadata:
  name: nginx-configuration 
```

Еще необходимо создать Service типа NodePort для доступа к внешнему миру:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: NodePort
  ports:
    - port: 80
      targetport: 80
      protocol: TCP
      name: http
    - port: 443
      targetport: 443
      protocol: TCP
      name: https
      
```

И для работы ingress controller так же необходим ServiceAccount с правильно настроенными ролями и привязкой роли.
```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ngin-controller-service-account
```

---
##### Ingress Resources

Набор правил которые вы настраиваете на Ingress Controller'е называется `Ingress Resources`.

Для этого создается файл с типом Ingress:

- С одним путем/хостом: 
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear
spec:
  backend:
    service:
      name: wear-service
      port: 
	    number: 80
```

- С несколькими путями
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port: 
              number: 80
      - path: /watch
        backend:
          service:
            name: watch-service
            port:
              number: 80
```

- С несколькими хостами
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - host: wear.hostname.com
    http:
      paths:
      - backend:
          service:
            name: wear-service
            port: 
              number: 80
  - host
    http:
      paths:
	  - backend:
	      service:
		    name: watch-service
		    port:
		      number: 80
```

- Мы можем создавать Ingress используя команды:
```bash
kubectl create ingress ingress-test \
	--rule="wear.my-online-store.com/wear*=wear-service:80"**
```